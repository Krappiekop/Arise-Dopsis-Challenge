{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Insect Classification Project\n",
    "### Project Overview\n",
    "The goal of this project is to develop an improved algorithm for the detection and classification of insect species using the DIOPSIS image processing pipeline. The primary objective is to accurately outline all insects visible on the screen for detection, and classify each insect by species. This project is part of the ARISE Diopsis Challenge, which aims to enhance biodiversity monitoring through automated insect identification.\n",
    "\n",
    "### Key Challenges:\n",
    "Detection:\n",
    "- Large number of insects per image.\n",
    "- Wide range of insect sizes (from a few millimeters to several centimeters).\n",
    "- Overlapping insects.\n",
    "- Presence of non-insect structures like vegetation, dirt, and shadows.\n",
    "\n",
    "Classification:\n",
    "- Imbalance in the number of training examples per species.\n",
    "- Fine-grained nature of the task.\n",
    "- Appropriate taxonomic level to output results.\n",
    "- Relatively poor image quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\Gebruiker\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.applications import ResNet50\n",
    "from keras.layers import Dense, GlobalAveragePooling2D\n",
    "from keras.models import Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation\n",
    "Loading the Dataset:\n",
    "- We load the classification_labels.csv and name_to_ancestors.csv files which provide the image labels and taxonomic hierarchy.\n",
    "\n",
    "Preprocessing Images:\n",
    "- All images are resized to a uniform size to facilitate consistent input into the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load classification labels\n",
    "classification_labels = pd.read_csv('Data/input/classification_labels.csv')\n",
    "name_to_ancestors = pd.read_csv('Data/input/name_to_ancestors.csv')\n",
    "\n",
    "# Add appropriate file extension if needed\n",
    "classification_labels['basename'] = classification_labels['basename'].apply(lambda x: x + '.jpg')  # Assuming .jpg extension\n",
    "\n",
    "# Correct directory path\n",
    "directory = 'Data/input/images_resized'\n",
    "\n",
    "# Constants\n",
    "IMG_SIZE = (128, 128)\n",
    "BATCH_SIZE = 32\n",
    "NUM_CLASSES = len(classification_labels['deepest_name'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training\n",
    "Data Augmentation:\n",
    "- We use ImageDataGenerator to augment the training data. This helps improve the model's robustness by introducing variations in the training images.\n",
    "\n",
    "Building the Model:\n",
    "- We use the ResNet50 model pre-trained on ImageNet and add custom layers for our classification task.\n",
    "- The model is trained with categorical cross-entropy loss and Adam optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 31556 validated image filenames belonging to 84 classes.\n",
      "Found 7889 validated image filenames belonging to 84 classes.\n"
     ]
    }
   ],
   "source": [
    "# Data augmentation and preprocessing\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    validation_split=0.2,\n",
    "    horizontal_flip=True,\n",
    "    zoom_range=0.2\n",
    ")\n",
    "\n",
    "train_generator = train_datagen.flow_from_dataframe(\n",
    "    dataframe=classification_labels,\n",
    "    directory=directory,\n",
    "    x_col='basename',\n",
    "    y_col='deepest_name',\n",
    "    target_size=IMG_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='categorical',\n",
    "    subset='training'\n",
    ")\n",
    "\n",
    "validation_generator = train_datagen.flow_from_dataframe(\n",
    "    dataframe=classification_labels,\n",
    "    directory=directory,\n",
    "    x_col='basename',\n",
    "    y_col='deepest_name',\n",
    "    target_size=IMG_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='categorical',\n",
    "    subset='validation'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\Gebruiker\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\backend.py:1398: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\Gebruiker\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\layers\\normalization\\batch_normalization.py:979: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\Gebruiker\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "Epoch 1/25\n",
      "WARNING:tensorflow:From c:\\Users\\Gebruiker\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\Gebruiker\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "987/987 [==============================] - 650s 655ms/step - loss: 2.4747 - accuracy: 0.2857 - val_loss: 2.4513 - val_accuracy: 0.2967\n",
      "Epoch 2/25\n",
      "987/987 [==============================] - 628s 636ms/step - loss: 2.3943 - accuracy: 0.3019 - val_loss: 2.3264 - val_accuracy: 0.3016\n",
      "Epoch 3/25\n",
      "987/987 [==============================] - 626s 634ms/step - loss: 2.2885 - accuracy: 0.3371 - val_loss: 2.2482 - val_accuracy: 0.4084\n",
      "Epoch 4/25\n",
      "987/987 [==============================] - 626s 634ms/step - loss: 2.2005 - accuracy: 0.3791 - val_loss: 2.2007 - val_accuracy: 0.3172\n",
      "Epoch 5/25\n",
      "987/987 [==============================] - 626s 634ms/step - loss: 2.1478 - accuracy: 0.3952 - val_loss: 2.1166 - val_accuracy: 0.4217\n",
      "Epoch 6/25\n",
      "987/987 [==============================] - 625s 633ms/step - loss: 2.1061 - accuracy: 0.4173 - val_loss: 2.0748 - val_accuracy: 0.4401\n",
      "Epoch 7/25\n",
      "987/987 [==============================] - 626s 634ms/step - loss: 2.0749 - accuracy: 0.4256 - val_loss: 2.1383 - val_accuracy: 0.3766\n",
      "Epoch 8/25\n",
      "987/987 [==============================] - 625s 633ms/step - loss: 2.0441 - accuracy: 0.4387 - val_loss: 2.0436 - val_accuracy: 0.4341\n",
      "Epoch 9/25\n",
      "987/987 [==============================] - 625s 633ms/step - loss: 2.0226 - accuracy: 0.4477 - val_loss: 2.0079 - val_accuracy: 0.4618\n",
      "Epoch 10/25\n",
      "987/987 [==============================] - 623s 632ms/step - loss: 2.0068 - accuracy: 0.4502 - val_loss: 2.0361 - val_accuracy: 0.4059\n",
      "Epoch 11/25\n",
      "987/987 [==============================] - 624s 632ms/step - loss: 1.9924 - accuracy: 0.4559 - val_loss: 2.0191 - val_accuracy: 0.4428\n",
      "Epoch 12/25\n",
      "987/987 [==============================] - 625s 633ms/step - loss: 1.9824 - accuracy: 0.4552 - val_loss: 1.9586 - val_accuracy: 0.4675\n",
      "Epoch 13/25\n",
      "987/987 [==============================] - 625s 633ms/step - loss: 1.9636 - accuracy: 0.4629 - val_loss: 1.9590 - val_accuracy: 0.4625\n",
      "Epoch 14/25\n",
      "987/987 [==============================] - 626s 635ms/step - loss: 1.9603 - accuracy: 0.4611 - val_loss: 1.9821 - val_accuracy: 0.4506\n",
      "Epoch 15/25\n",
      "987/987 [==============================] - 627s 635ms/step - loss: 1.9439 - accuracy: 0.4685 - val_loss: 1.9463 - val_accuracy: 0.4669\n",
      "Epoch 16/25\n",
      "987/987 [==============================] - 622s 631ms/step - loss: 1.9395 - accuracy: 0.4666 - val_loss: 2.0463 - val_accuracy: 0.4144\n",
      "Epoch 17/25\n",
      "987/987 [==============================] - 623s 631ms/step - loss: 1.9274 - accuracy: 0.4721 - val_loss: 1.9717 - val_accuracy: 0.4566\n",
      "Epoch 18/25\n",
      "987/987 [==============================] - 622s 631ms/step - loss: 1.9265 - accuracy: 0.4719 - val_loss: 1.9838 - val_accuracy: 0.4355\n",
      "Epoch 19/25\n",
      "987/987 [==============================] - 625s 633ms/step - loss: 1.9226 - accuracy: 0.4722 - val_loss: 1.9220 - val_accuracy: 0.4748\n",
      "Epoch 20/25\n",
      "987/987 [==============================] - 624s 632ms/step - loss: 1.9123 - accuracy: 0.4762 - val_loss: 1.9572 - val_accuracy: 0.4634\n",
      "Epoch 21/25\n",
      "987/987 [==============================] - 622s 630ms/step - loss: 1.9071 - accuracy: 0.4740 - val_loss: 1.9694 - val_accuracy: 0.4392\n",
      "Epoch 22/25\n",
      "987/987 [==============================] - 623s 631ms/step - loss: 1.8973 - accuracy: 0.4776 - val_loss: 2.0218 - val_accuracy: 0.4533\n",
      "Epoch 23/25\n",
      "987/987 [==============================] - 623s 631ms/step - loss: 1.8928 - accuracy: 0.4795 - val_loss: 1.8889 - val_accuracy: 0.4826\n",
      "Epoch 24/25\n",
      "987/987 [==============================] - 622s 630ms/step - loss: 1.8897 - accuracy: 0.4802 - val_loss: 1.9488 - val_accuracy: 0.4449\n",
      "Epoch 25/25\n",
      "987/987 [==============================] - 622s 631ms/step - loss: 1.8898 - accuracy: 0.4787 - val_loss: 1.9230 - val_accuracy: 0.4544\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Gebruiker\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "# Model selection\n",
    "base_model = ResNet50(include_top=False, weights='imagenet', input_shape=(128, 128, 3))\n",
    "x = GlobalAveragePooling2D()(base_model.output)\n",
    "x = Dense(1024, activation='relu')(x)\n",
    "predictions = Dense(NUM_CLASSES, activation='softmax')(x)\n",
    "\n",
    "model = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Training the model\n",
    "model.fit(\n",
    "    train_generator,\n",
    "    epochs=25,\n",
    "    validation_data=validation_generator\n",
    ")\n",
    "\n",
    "# Save model\n",
    "model.save('insect_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction and Formatting\n",
    "Generating Predictions:\n",
    "- The trained model is used to predict the species of insects in the test images.\n",
    "\n",
    "Formatting Predictions:\n",
    "- The predictions are formatted to match the hierarchical structure required by the challenge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns in name_to_ancestors:\n",
      "Index(['name', 'ancestors'], dtype='object')\n",
      "First few rows in name_to_ancestors:\n",
      "          name                               ancestors\n",
      "0     Animalia                            ['Animalia']\n",
      "1      Insecta                 ['Insecta', 'Animalia']\n",
      "2  Hymenoptera  ['Hymenoptera', 'Insecta', 'Animalia']\n",
      "3  Lepidoptera  ['Lepidoptera', 'Insecta', 'Animalia']\n",
      "4      Diptera      ['Diptera', 'Insecta', 'Animalia']\n"
     ]
    }
   ],
   "source": [
    "# Debug: Print columns and first few rows\n",
    "print(\"Columns in name_to_ancestors:\")\n",
    "print(name_to_ancestors.columns)\n",
    "print(\"First few rows in name_to_ancestors:\")\n",
    "print(name_to_ancestors.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 39445 validated image filenames.\n",
      "39445/39445 [==============================] - 4127s 105ms/step\n",
      "Formatted predictions saved to c:\\Users\\Gebruiker\\Documents\\Fontys\\S6 - AI\\Personal - Data Driven Challenge\\arise-challenge-algorithm\\mnt\\data\\formatted_predictions.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import ast\n",
    "import pandas as pd\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import load_model\n",
    "\n",
    "# Load the CSV files\n",
    "classification_labels = pd.read_csv('Data/input/classification_labels.csv')\n",
    "name_to_ancestors = pd.read_csv('Data/input/name_to_ancestors.csv')\n",
    "\n",
    "# Load model\n",
    "model = load_model('insect_model.h5')\n",
    "\n",
    "# Add .jpg extension to basenames if not already present\n",
    "classification_labels['basename'] = classification_labels['basename'].apply(lambda x: x if x.endswith('.jpg') else x + '.jpg')\n",
    "\n",
    "# Function to parse ancestors column\n",
    "def parse_ancestors(ancestors):\n",
    "    ancestors = ast.literal_eval(ancestors)\n",
    "    levels = {}\n",
    "    for i, level in enumerate(ancestors):\n",
    "        levels[f'level_{i}'] = level\n",
    "        levels[f'level_{i}_probability'] = None  # Placeholder for probabilities\n",
    "    return levels\n",
    "\n",
    "# Apply parsing to name_to_ancestors dataframe\n",
    "ancestors_df = name_to_ancestors['ancestors'].apply(parse_ancestors)\n",
    "ancestors_df = pd.DataFrame(list(ancestors_df))\n",
    "\n",
    "# Combine with name_to_ancestors dataframe\n",
    "name_to_ancestors = pd.concat([name_to_ancestors['name'], ancestors_df], axis=1)\n",
    "\n",
    "# Define function to format predictions\n",
    "def format_predictions(model, dataframe, name_to_ancestors):\n",
    "    # Generate predictions\n",
    "    image_dir = 'Data/input/images_resized'\n",
    "    datagen = ImageDataGenerator(rescale=1./255)\n",
    "    generator = datagen.flow_from_dataframe(\n",
    "        dataframe=dataframe,\n",
    "        directory=image_dir,\n",
    "        x_col='basename',\n",
    "        y_col=None,\n",
    "        target_size=(224, 224),\n",
    "        batch_size=1,\n",
    "        class_mode=None,\n",
    "        shuffle=False\n",
    "    )\n",
    "    \n",
    "    # Get predictions\n",
    "    predictions = model.predict(generator, steps=len(generator), verbose=1)\n",
    "    predicted_classes = predictions.argmax(axis=-1)\n",
    "    class_probabilities = predictions.max(axis=-1)\n",
    "    \n",
    "    # Add predictions to dataframe\n",
    "    dataframe['predicted_class'] = predicted_classes.astype(str)  # Convert to string for merging\n",
    "    dataframe['confidence'] = class_probabilities\n",
    "    \n",
    "    # Ensure name column in name_to_ancestors is string\n",
    "    name_to_ancestors['name'] = name_to_ancestors['name'].astype(str)\n",
    "    \n",
    "    # Merge with ancestors to get the full hierarchy\n",
    "    formatted_predictions = dataframe.merge(name_to_ancestors, left_on='predicted_class', right_on='name')\n",
    "    \n",
    "    # Format according to the challenge requirements\n",
    "    for i in range(6):\n",
    "        formatted_predictions[f'level_{i}_probability'] = formatted_predictions['confidence']  # Use the prediction confidence for all levels\n",
    "    \n",
    "    formatted_predictions = formatted_predictions[['basename', 'level_0', 'level_0_probability', \n",
    "                                                   'level_1', 'level_1_probability', \n",
    "                                                   'level_2', 'level_2_probability', \n",
    "                                                   'level_3', 'level_3_probability', \n",
    "                                                   'level_4', 'level_4_probability', \n",
    "                                                   'level_5', 'level_5_probability']]\n",
    "    \n",
    "    return formatted_predictions\n",
    "\n",
    "# Apply the formatting function\n",
    "formatted_predictions = format_predictions(model, classification_labels, name_to_ancestors)\n",
    "\n",
    "# Save to CSV\n",
    "output_file_path = 'mnt/data/formatted_predictions.csv'\n",
    "absolute_path = os.path.abspath(output_file_path)\n",
    "formatted_predictions.to_csv(absolute_path, index=False)\n",
    "print(f\"Formatted predictions saved to {absolute_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation and Results\n",
    "Evaluation:\n",
    "- Compare the model's predictions with the ground truth to evaluate its accuracy and reliability.\n",
    "\n",
    "Results:\n",
    "- The formatted predictions are saved to formatted_predictions.csv for submission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 39445 validated image filenames.\n",
      "39445/39445 [==============================] - 2915s 74ms/step\n"
     ]
    }
   ],
   "source": [
    "# Evaluate model and save results\n",
    "def evaluate_and_save_results(model, csv_file, image_dir, output_file):\n",
    "    test_labels = pd.read_csv(csv_file)\n",
    "    test_labels.columns = test_labels.columns.str.strip()\n",
    "    \n",
    "    if 'basename' not in test_labels.columns:\n",
    "        raise KeyError(\"Column 'basename' does not exist in the predictions file.\")\n",
    "    \n",
    "    test_labels['basename'] = test_labels['basename'].apply(lambda x: x if x.endswith('.jpg') else x + '.jpg')\n",
    "    \n",
    "    missing_files = []\n",
    "    for fname in test_labels['basename']:\n",
    "        if not os.path.exists(os.path.join(image_dir, fname)):\n",
    "            missing_files.append(fname)\n",
    "    \n",
    "    if missing_files:\n",
    "        raise FileNotFoundError(f\"The following files are missing: {missing_files}\")\n",
    "    \n",
    "    test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "    test_generator = test_datagen.flow_from_dataframe(\n",
    "        dataframe=test_labels,\n",
    "        directory=image_dir,\n",
    "        x_col='basename',\n",
    "        y_col=None,\n",
    "        target_size=IMG_SIZE,\n",
    "        batch_size=1,\n",
    "        class_mode=None,\n",
    "        shuffle=False\n",
    "    )\n",
    "    \n",
    "    if len(test_generator) == 0:\n",
    "        raise ValueError(\"Test generator is empty. No images found.\")\n",
    "    \n",
    "    predictions = model.predict(test_generator, steps=len(test_generator), verbose=1)\n",
    "    predicted_classes = predictions.argmax(axis=-1)\n",
    "    \n",
    "    test_labels['predictions'] = predicted_classes\n",
    "    test_labels.to_csv(output_file, index=False)\n",
    "    \n",
    "# Assuming test images are in 'Data/input/images_resized'\n",
    "evaluate_and_save_results(model, 'mnt/data/predictions.csv', 'Data/input/images_resized', 'mnt/data/formatted_predictions.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interim Conclusion and Future Work\n",
    "### Summary of Work Completed\n",
    "In this project, we have developed an initial version of an insect classification system as part of the ARISE Diopsis Challenge. Here are the key steps we have completed:\n",
    "\n",
    "1. Data Preparation:\n",
    "\n",
    "    - Loaded and preprocessed the insect image dataset.\n",
    "    - Ensured all images are resized to a uniform size.\n",
    "    - Verified and merged the classification labels with the taxonomic hierarchy.\n",
    "\n",
    "2. Model Training:\n",
    "\n",
    "    - Utilized the pre-trained ResNet50 model, fine-tuned with additional layers for insect species classification.\n",
    "    - Implemented data augmentation techniques to enhance model robustness.\n",
    "    - Trained the model using the training dataset and validated its performance on a separate validation set.\n",
    "\n",
    "3. Prediction and Formatting:\n",
    "\n",
    "    - Generated predictions using the trained model on the test dataset.\n",
    "    - Formatted the predictions according to the hierarchical structure required by the challenge.\n",
    "    - Saved the formatted predictions in the specified format for submission.\n",
    "\n",
    "### Current Results\n",
    "The initial model has successfully processed the dataset and generated predictions for insect species classification. The formatted predictions have been saved in the required format, ready for evaluation. This initial implementation serves as a solid foundation for further improvements and refinements.\n",
    "\n",
    "### Future Work\n",
    "Despite the progress made, there are several areas where we can further enhance the performance and accuracy of our model:\n",
    "\n",
    "1. Addressing Class Imbalance:\n",
    "\n",
    "    - Implement techniques to handle the imbalance in the number of training examples per species. This could include data augmentation, synthetic data generation, or using specialized loss functions that mitigate class imbalance.\n",
    "\n",
    "2. Improving Model Accuracy:\n",
    "\n",
    "    - Experiment with different neural network architectures and hyperparameters to improve model accuracy.\n",
    "    - Fine-tune the pre-trained model more extensively with more epochs and different learning rates.\n",
    "\n",
    "3. Enhanced Data Augmentation:\n",
    "\n",
    "    - Apply more advanced data augmentation techniques to create a more diverse training dataset, potentially improving the model's ability to generalize.\n",
    "\n",
    "4. Incorporating Additional Data:\n",
    "\n",
    "    - Explore the possibility of incorporating additional datasets or external data sources to provide more training examples and enhance model performance.\n",
    "\n",
    "5. Model Evaluation and Fine-Tuning:\n",
    "\n",
    "    - Conduct a thorough evaluation of the model's predictions against the ground truth.\n",
    "    - Fine-tune the model based on the evaluation results to address any weaknesses or inaccuracies.\n",
    "\n",
    "6. Pipeline Optimization:\n",
    "\n",
    "    - Optimize the overall image processing and classification pipeline for efficiency and scalability.\n",
    "    - By focusing on these areas, we aim to develop a more robust and accurate insect classification system that meets the stringent requirements of the ARISE Diopsis Challenge. Our ultimate goal is to contribute to biodiversity monitoring efforts through automated and precise insect identification.\n",
    "\n",
    "### Next Steps\n",
    "- Conduct a detailed analysis of the current model's performance, identifying specific areas for improvement.\n",
    "- Implement and test various techniques to address class imbalance.\n",
    "- Experiment with different neural network architectures and fine-tune hyperparameters.\n",
    "- Apply more advanced data augmentation techniques.\n",
    "- Evaluate the model thoroughly and refine it based on the findings.\n",
    "- Document the entire process, including methodologies, results, and improvements, for the final project submission."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
